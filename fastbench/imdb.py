# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/01_imdb.ipynb (unless otherwise specified).

__all__ = ['get_dls', 'train', 'train_imdb']

# Cell
from fastai.basics import *
from fastai.callback.all import *
from fastai.distributed import *
from fastprogress import fastprogress
from fastai.callback.mixup import *
from fastscript import *
from fastai.text.all import *

torch.backends.cudnn.benchmark = True
fastprogress.MAX_COLS = 80

# Cell
def get_dls(bs=64):
    path = rank0_first(lambda:untar_data(URLs.IMDB))
    return TextDataLoaders.from_folder(path, bs=bs, valid='test')

# Cell
def train(
    gpu=None,
    lr=1e-2,
    bs=64,
    epochs=5,
    fp16=0,
    dump=0,
    runs=1,
):
    "Training of IMDB classifier."

    if torch.cuda.is_available():
        n_gpu = torch.cuda.device_count()
        if gpu is None: gpu = list(range(n_gpu))[0]
        torch.cuda.set_device(gpu)
    else:
        n_gpu = None

    dls = get_dls(bs)

    for run in range(runs):
        print(f'Rank[{rank_distrib()}] Run: {run}; epochs: {epochs}; lr: {lr}; bs: {bs}')

        learn = rank0_first(lambda: text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy))

        if dump: print(learn.model); exit()
        if fp16: learn = learn.to_fp16()

        if num_distrib() > 1 and torch.__version__.startswith("1.4"): DistributedTrainer.fup = True

        with learn.distrib_ctx(cuda_id=gpu): # distributed traing requires "-m fastai.launch"
            print(f"Training in distributed data parallel context on GPU {gpu}", flush=True)
            learn.fine_tune(epochs, lr)

# Cell
def train_imdb(gpu=None, bs=64, fp16=0, epochs=5, runs=1):
    train(gpu=gpu, bs=bs, fp16=fp16, epochs=epochs,  runs=runs)