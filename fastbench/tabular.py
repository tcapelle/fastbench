# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02_tabular.ipynb (unless otherwise specified).

__all__ = ['get_dls', 'train', 'train_tabular']

# Cell
def get_dls(bs=64):
    path = untar_data(URLs.ADULT_SAMPLE)
    dls = TabularDataLoaders.from_csv(path/'adult.csv', path=path, y_names="salary",
        cat_names = ['workclass', 'education', 'marital-status', 'occupation',
                 'relationship', 'race'],
        cont_names = ['age', 'fnlwgt', 'education-num'],
        procs = [Categorify, FillMissing, Normalize],
        bs=bs)
    return dls

# Cell
def train(
    gpu=None,
    bs=64,
    epochs=5,
    fp16=0,
    dump=0,
    runs=1,
):
    "Training of Tabular data 'ADULT_SAMPLE'."

    # gpu = setup_distrib(gpu)
    if gpu is not None: torch.cuda.set_device(gpu)

    path = untar_data(URLs.ADULT_SAMPLE)
    dls = get_dls(bs)

    if not gpu: print(f'epochs: {epochs};')

    for run in range(runs):
        print(f'Run: {run}')

        learn = tabular_learner(dls, metrics=accuracy)
        if dump: print(learn.model); exit()
        if fp16: learn = learn.to_fp16()

        n_gpu = torch.cuda.device_count()
        ctx = learn.parallel_ctx if gpu is None and n_gpu else learn.distrib_ctx

        with partial(ctx, gpu)(): # distributed traing requires "-m fastai.launch"
            print(f"Training in {ctx.__name__} context on GPU {gpu if gpu is not None else list(range(n_gpu))}")
            learn.fit_one_cycle(epochs)

# Cell
def train_tabular(gpu=None, bs=64, fp16=0, epochs=5, runs=1):
    train(gpu=gpu, bs=bs, fp16=fp16, epochs=epochs, runs=runs)